# import os
# import json
# from tqdm import tqdm   # libreria para mostrar progresos en python
# import typing
# from pathlib import Path    # con esto referenciamos a la ruta del proyecto
# import streamlit as st


# """
# Este script nos permite convertir las anotaciones del dataset de formato COCO -> Common Objects in Context
# a formato YOLO.

# El dataset ya tiene divididos las imagenes en train/validation/test y nos facilita unos .txt junto con las annotations,
# usaremos estos .txt para distribuir las anotaciones entre cada una de nuestras clases. 

# """

# # Configurar rutas

# # Crear carpetas /labels/train, /val, /test

# # Para cada split (train, val, test):
# #     Cargar lista de im√°genes del split

# #     Para cada imagen del split:
# #         Cargar su json de anotaciones
# #         Leer width y height de la imagen

# #         Para cada objeto anotado:
# #             Leer label
# #             Leer bbox (xmin, ymin, xmax, ymax)
# #             Convertir bbox a (x_center, y_center, w, h) normalizados
# #             Mapear label a id num√©rico
# #             Escribir l√≠nea en formato YOLO

# # Guardar todas las l√≠neas en un archivo .txt por imagen

# # Crear data.yaml al final


# BASE_DIR = Path(__file__).resolve().parent
# DATA_DIR = BASE_DIR / '../data/raw'
# DATA_DIR = DATA_DIR.resolve()

# lista_tipos_datos = ['labels', 'images']
# lista_split = ['train', 'val', 'test']


# # creo las 3 carpetas en las que dividire mis labels
# for dato in lista_tipos_datos:
#     for carpeta in lista_split:
#         (DATA_DIR / dato / carpeta).mkdir(parents=True, exist_ok=True)
# """Se que es un doble bucle pero no hay riesgo de que sea infinito. Es horrible pero efectivo"""

# SPLITS_DIR = DATA_DIR / './COCO_Annotation/splits'
# SPLITS_DIR = SPLITS_DIR.resolve()

# splits = {}  # inicializo mi diccionario que contendra las ids de train/val/test

# for name in lista_split:
#     split_file = SPLITS_DIR / (name + '.txt')
#     with open(split_file, 'r') as f:
#         splits[name] = [ln.strip() for ln in f if ln.strip()]


# print(splits.keys())  # compruebo que me este extrayendo correctamente los nombres


# """
# En los .json las cajas de Object Location, indican los pixeles absolutos de la imagen para:
#     -xmin
#     -ymin
#     -xmax
#     -ymax

# YOLO necesita que esten en otro formato tal que: <class_id> <x_center> <y_center> <width> <height>
# Con los valores normalizados entre 0 y 1, no pixeles absolutos.

# En la siguiente funcion realizare la conversion
#     """

# def coco2yolo(box: dict, ancho: int, alto: int)-> typing.Tuple[float, float, float, float]:
#     """
#     Siendo las variables aceptadas box, ancho y alto
#         - box: diccionario bbox del archivo .json
#         - ancho: dimension x de mi imagen 
#         - alto: dimension y de mi imagen
#     """
#     xmin, ymin, xmax, ymax = box['xmin'], box['ymin'], box['xmax'], box['ymax'] # extraigo las coordenadas del .json
#     x_centro = (xmin + xmax) / 2 / ancho      
#     y_centro = (ymin + ymax) / 2 / alto      
#     wide   = (xmax - xmin) / ancho          
#     height   = (ymax - ymin) / alto          
#     return x_centro, y_centro, wide, height


# """
# En mis .json, tengo el tipo de senal relacionada con un numero.

# Tengo que sacar estos tipos junto con su numero y meterlos en un diccionario.

# Mas adelante necesitare pasarlos a una lista en la que los indices sean los numeros enteros y esta
# estructura de datos es la que le servira a YOLO.

# No creo una lista directamente porque seria muy poco efectivo desde un punto de vista computacional,
# habria que recorrer dicha lista por cada imagen que me encuentre, buscando si ya he apuntado ese label.
# """


# # inicializo mi diccionario que relacionara label con el numero del json
# ANN_DIR = DATA_DIR / './COCO_Annotation/annotations/'
# ANN_DIR = ANN_DIR.resolve()

# label2id = {}
# id2label = []


# """ 
# Para cada split ('train', 'val', 'test'):
#     Para cada imagen en el split:
#         Construir la ruta al archivo JSON de la imagen
#         Construir la ruta de destino del archivo .txt

#         Abrir y leer el JSON de anotaciones
#         Obtener el ancho y alto de la imagen

#         Inicializar una lista vac√≠a para guardar las l√≠neas YOLO

#         Para cada objeto anotado en la imagen:
#             Si el objeto es falso (dummy):
#                 Saltarlo

#             Leer el nombre de la clase (label)

#             Si la clase no existe en el diccionario:
#                 Asignarle un nuevo n√∫mero de clase
#                 Guardar su nombre en la lista de clases

#             Convertir el bounding box a formato YOLO
#                 (calcular centro x, centro y, ancho, alto normalizados)

#             Crear una l√≠nea con el formato YOLO
#             A√±adir esa l√≠nea a la lista

#         Escribir todas las l√≠neas en un archivo .txt correspondiente a la imagen

#  """

# archivos_faltantes = 0  # <-- MUY IMPORTANTE

# for split, ids in splits.items():
#     if split == 'test':
#         continue

#     for img_id in tqdm(ids, desc=f'{split:5}', ncols=80):
#         ann_path = ANN_DIR / f'{img_id}.json'

#         if not ann_path.exists():
#             print(f"[ADVERTENCIA] No se encontr√≥ el archivo: {ann_path.name} en la ruta: {ann_path}")
#             archivos_faltantes += 1
#             continue

#         try:
#             with open(ann_path) as f:
#                 ann = json.load(f)
#         except json.JSONDecodeError:
#             print(f"[ADVERTENCIA] Error al decodificar JSON para el archivo: {ann_path.name} en la ruta: {ann_path}")
#             continue
#         except Exception as e:
#             print(f"[ADVERTENCIA] Error inesperado al intentar leer el archivo: {ann_path.name} en la ruta: {ann_path}: {e}")
#             continue

#         if 'width' not in ann or 'height' not in ann:
#             print(f"[ADVERTENCIA] El archivo {ann_path.name} no contiene las claves 'width' o 'height'.")
#             continue

#         # leo las dimensiones de la imagen en el .json
#         ancho, alto = ann['width'], ann['height']

#         # inicializo la lista donde guardare los datos
#         YOLO_data = []

#         for obj in ann['objects']:    # objects es el nombre de la key en el diccionario .json
#             label = obj['label']      # extraigo el label

#             if label not in label2id:
#                 label2id[label] = len(label2id)  # voy a usar la longitud de la lista, asi siempre me creara el siguiente integer.
#                 id2label.append(label)           # lo guardo en mi lista
        
#             # extraigo el id de cada foto
#             cls_id = label2id[label]

#             x_center, y_center, bbox_ancho, bbox_alto = coco2yolo(obj['bbox'], ancho, alto)   # paso la bbox del json y las dimensiones de la foto a mi funcion coco2yolo

#             # Creo mis lineas en formato YOLO
#             line = f"{cls_id} {x_center:.6f} {y_center:.6f} {bbox_ancho:.6f} {bbox_alto:.6f}"
#             YOLO_data.append(line)

#         # guardo toda la info transformada en un .txt para  YOLO
#         txt_path = DATA_DIR / 'labels' / split / f'{img_id}.txt'
#         with open(txt_path, 'w') as f:
#             f.write('\n'.join(YOLO_data))

# print(f"\nProceso terminado ‚úÖ")
# print(f"Archivos de anotaciones faltantes: {archivos_faltantes}")

# # tengo que crear un archivo yaml donde guardare los datos de mis etiquetas
# yaml_path = DATA_DIR / 'labels' / 'data.yaml'
# with open(yaml_path, 'w') as f:
#     f.write(f"path: {DATA_DIR}\n")
#     f.write("train: images/train\nval: images/val\ntest: images/test\n")
#     f.write(f"nc: {len(id2label)}\n")
#     f.write("names:\n")
#     for i, name in enumerate(id2label):
#         f.write(f"  {i}: {name}\n")

# print("COCO to YOLO conversion finalizada.  Data YAML en:", yaml_path)


# TESTING STREAMLIT 

# import streamlit as st
# from PIL import Image
# import numpy as np
# import tempfile
# import cv2
# from ultralytics import YOLO
# # Configuraci√≥n de la p√°gina

# st.set_page_config(page_title="Detector de Se√±ales de Tr√°fico", layout="wide")
# # T√≠tulo
# st.title(":vertical_traffic_light: Detectar y Clasificar Se√±ales de Tr√°fico")
# st.markdown("Sube una imagen y el modelo detectar√° las se√±ales de tr√°fico presentes en ella.")
# Cargar modelo YOLO
# @st.cache_resource
# def cargar_modelo():
#     modelo = YOLO("/Users/johncarter/Documents/GitHub/Detector-Senales-Trafico/data/raw/labels/data.yaml")  # Cambia por la ruta real de tu modelo entrenado
#     return modelo
# modelo = cargar_modelo()
# Subida de imagen
# uploaded_file = st.file_uploader(":camera: Sube una imagen", type=["jpg", "jpeg", "png"])
# if uploaded_file is not None:
#     imagen = Image.open(uploaded_file).convert("RGB")
#     st.image(imagen, caption=":camera: Imagen subida", use_column_width=True)
#     if st.button(":mag: Detectar Se√±ales"):
#         with st.spinner("Procesando..."):
#             # Guardar temporalmente la imagen
#             with tempfile.NamedTemporaryFile(delete=False, suffix=".jpg") as tmp:
#                 imagen.save(tmp.name)
#                 resultados = modelo(tmp.name)
#             # Obtener la imagen con los bounding boxes
#             img_array = resultados[0].plot()  # Devuelve una imagen con las detecciones dibujadas
#             st.image(img_array, caption=":brain: Resultado del modelo", use_column_width=True)
#             # Mostrar tabla de resultados
#             boxes = resultados[0].boxes
#             if boxes is not None and len(boxes) > 0:
#                 clases = boxes.cls.cpu().numpy().astype(int)
#                 confianzas = boxes.conf.cpu().numpy()
#                 nombres = [modelo.names[i] for i in clases]
#                 st.subheader(":bar_chart: Se√±ales detectadas")
#                 for nombre, conf in zip(nombres, confianzas):
#                     st.write(f"- {nombre} ({conf*100:.1f}%)")
#             else:
#                 st.info("No se detectaron se√±ales en la imagen.")
# # Secci√≥n informativa
# # Secci√≥n informativa
# with st.expander(":information_source: Sobre este proyecto"):
#     st.markdown("""
#     - **Modelo:** YOLOv5/YOLOv8 entrenado con el dataset MTSD (Mapillary Traffic Sign Dataset).
#     - **Objetivo:** Detectar y clasificar se√±ales de tr√°fico a partir de im√°genes.
#     - **Equipo:** Proyecto de grupo en Machine Learning.
#     - **Frameworks:** Streamlit, PyTorch, Ultralytics YOLO.
#     """)

#2nd TRY

import streamlit as st
import cv2
import numpy as np
from PIL import Image
import tempfile
import os
# # Configuraci√≥n inicial de la p√°gina
# st.set_page_config(page_title="Detector de Se√±ales de Tr√°fico", layout="wide")
# # T√≠tulo y descripci√≥n
# st.header(":vertical_traffic_light: Detectar y Clasificar Se√±ales de Tr√°fico")

# st.markdown("""
# Sube una imagen y nuestro modelo entrenado con YOLO detectar√° y clasificar√° las se√±ales de tr√°fico.
# """)

# col1, col2 = st.columns(2)
# col1.write('Column 1')
# col2.write('Column 2')

# # Carga del archivo de imagen
# uploaded_file = st.file_uploader(":camera: Sube una imagen", type=["jpg", "jpeg", "png"])

# if uploaded_file is not None:
#     image = Image.open(uploaded_file).convert("RGB")
#     image_np = np.array(image)
#     st.image(image, caption="Imagen subida", use_column_width=True)
#     # Bot√≥n para ejecutar detecci√≥n
#     if st.button(":mag: Detectar Se√±ales"):
#         with st.spinner("Detectando se√±ales..."):
#             # Guardar la imagen temporalmente
#             with tempfile.NamedTemporaryFile(delete=False, suffix='.jpg') as tmp_file:
#                 image.save(tmp_file.name)
#                 image_path = tmp_file.name
#             # ---------- L√≥gica de detecci√≥n (YOLO) -----------
#             # Aqu√≠ debes ejecutar tu modelo YOLO y guardar la imagen de salida con cajas
#             # Por ejemplo, si usas YOLOv5 con PyTorch:
#             #
#             # from yolov5 import YOLOv5
#             # yolo = YOLOv5("path/to/best.pt", device="cpu")
#             # results = yolo.predict(image_path)
#             # results.save("output.jpg")
#             #
#             # Para el ejemplo, solo copiamos la imagen original
#             output_path = os.path.join(tempfile.gettempdir(), "output.jpg")
#             cv2.imwrite(output_path, cv2.cvtColor(image_np, cv2.COLOR_RGB2BGR))
#             # Leer imagen de salida
#             output_image = Image.open(output_path)
#             st.success("¬°Detecci√≥n completada!")
#             st.image(output_image, caption="Resultado de la detecci√≥n", use_column_width=True)
#             # Resultados simulados (reemplaza con tus resultados reales)
#             st.subheader(":bar_chart: Resultados de la detecci√≥n")
#             st.dataframe({
#                 "Clase": ["Se√±al de Stop", "L√≠mite 50"],
#                 "Confianza": [0.98, 0.91],
#                 "Posici√≥n": [(120, 60, 180, 120), (200, 100, 250, 150)]
#             })
# # Informaci√≥n del modelo
# with st.expander(":information_source: Sobre el modelo"):
#     st.markdown("""
#     - **Modelo**: YOLOv5 entrenado con el dataset MTSD
#     - **Clases detectadas**: advertencia, prohibici√≥n, obligaci√≥n, informaci√≥n
#     - **Framework**: PyTorch + OpenCV
#     """)


    # 3 TRY

import streamlit as st

# Configuraci√≥n inicial
st.set_page_config(page_title="Decode Traffic Signs", layout="wide")

# Estilos personalizados
st.markdown("""
    <style>
    .stApp {
        background-color: #000000; /* Color de fondo del cuerpo de la app (Negro) */
        padding: 2rem; /* Espacio alrededor de la caja principal */
    }

    /* Estilo para el contenedor principal que envuelve todo el contenido */
    div[data-testid="stVerticalBlock"] { /* Este es el main container creado por st.container() */
        background-color: #FFFAF0; /* Fondo de la caja principal (Floral White) */
        padding: 2rem; /* Relleno dentro de la caja principal */
        border-radius: 15px; /* Bordes redondeados */
        box-shadow: 0 0 10px rgba(0,0,0,0.5); /* Sombra sutil */
    }

    /* Asegurar que los elementos dentro del contenedor principal tengan espaciado */
    div[data-testid="stVerticalBlock"] > h1 {
        margin-bottom: 0.2em;
    }
    div[data-testid="stVerticalBlock"] > div.intro-text {
        margin-bottom: 2em;
    }

    /* Estilo para el contenedor que tiene las columnas (el padre de col1 y col2) */
    div[data-testid="stVerticalBlock"] > div[data-testid="stHorizontalBlock"] {
        margin-top: 1.5rem; /* Espacio encima de las columnas */
        gap: 2rem; /* Espacio entre las columnas */
        display: flex; /* Lo convertimos en un contenedor flex */
        align-items: stretch; /* ¬°Importante! Hace que los hijos (las columnas) se estiren a la misma altura */
    }

    /* Estilo para cada columna individual */
    div[data-testid="stVerticalBlock"] > div[data-testid="stHorizontalBlock"] > div[data-testid="stVerticalBlock"] {
        /* Este es cada div de columna (col1, col2) */
        display: flex; /* Convertimos la columna en un contenedor flex */
        flex-direction: column; /* Apilamos el contenido verticalmente dentro de la columna */
        /* height: 100%; */ /* No es necesario si usamos flex-grow en los hijos y el padre se estira */
    }

    /* Estilo para las cajas de informaci√≥n y del uploader dentro de las columnas */
    .info-box,
    .file-uploader-box {
        flex-grow: 1; /* ¬°Importante! Hace que estas cajas se expandan para llenar la altura disponible en su columna */
        /* height: 100%; */ /* Puede que no sea necesario con flex-grow */
        box-sizing: border-box; /* Asegura que el padding se incluya en el tama√±o total */
        padding: 20px;
        border-radius: 10px;
        /* Los colores de fondo espec√≠ficos se definen a continuaci√≥n */
    }

    /* Colores de fondo espec√≠ficos para las cajas */
    .info-box {
         background-color: #f0f2f6; /* Color gris claro */
    }
    .file-uploader-box {
        background-color: #ffffff; /* Color blanco */
        border: 2px dashed #ccc;
        text-align: center;
    }

    /* Ajuste para el contenido dentro de file-uploader-box para ayudar al centrado vertical */
    .file-uploader-box > div { /* Apunta al div de markdown dentro de file-uploader-box */
         display: flex;
         flex-direction: column;
         align-items: center;
         justify-content: center;
         height: 100%; /* Asegura que este div llene su contenedor padre (.file-uploader-box) */
         box-sizing: border-box;
    }

    /* Espacio encima del expander */
    div[data-testid="stVerticalBlock"] > div[data-testid="stExpander"] {
        margin-top: 1.5rem;
    }

    /* Puedes necesitar ajustar el estilo del widget st.file_uploader en s√≠ mismo
       para que su √°rea de "drag and drop" tambi√©n se adapte a la altura si es necesario,
       pero esto a veces es complejo con los componentes internos de Streamlit.
       La altura de la caja principal del uploader ya se controla con flex-grow.
    */

    </style>
""", unsafe_allow_html=True)

# Usa st.container() para crear el contenedor principal
# Todo lo que est√© dentro de este 'with' block estar√° contenido en este div
with st.container():
    # T√≠tulo centrado
    st.markdown("<h1>üö¶ Decode Traffic Signs</h1>", unsafe_allow_html=True)

    # Texto explicativo centrado
    st.markdown("""
    <div class='intro-text'>
        Este modelo <strong>YOLOv8</strong> te ayudar√° a identificar y clasificar se√±ales de tr√°fico,
        las cuales categorizar√° en las siguientes clases detectadas:
        <strong>Advertencia</strong>, <strong>Prohibici√≥n</strong>, <strong>Obligaci√≥n</strong> e <strong>Informaci√≥n</strong>.
    </div>
    """, unsafe_allow_html=True)

    # T√≠tulo encima de las columnas
    st.markdown("### üì∏ Subir una imagen")

    # Columnas sim√©tricas
    col1, col2 = st.columns(2)

    with col1:
        st.markdown("""
        <div class='info-box'>
            <strong>YoloV8</strong><br><br>
            Nuestro modelo entrenado con YOLO detectar\u00E1 y clasificar\u00E1 las se\u00F1ales de tr\u00E1fico presentes en ella.
        </div>
        """, unsafe_allow_html=True)

    with col2:
        # Envuelve el texto del file uploader en la clase file-uploader-box
        # El widget st.file_uploader se renderizar√° despu√©s de este div de markdown, dentro de la columna.
        # El flex-grow en .file-uploader-box har√° que esta caja se expanda,
        # y el widget de Streamlit se ubicar√° dentro de esa caja expandida.
        st.markdown("""
        <div class='file-uploader-box'>
            <p>Usa el siguiente campo para arrastrar y soltar tu imagen o haz clic para seleccionarla.</p>
        </div>
        """, unsafe_allow_html=True)
        uploaded_file = st.file_uploader("", type=["jpg", "jpeg", "png"])


    # Desplegable con m√°s informaci√≥n
    with st.expander("‚¨áÔ∏è M√°s info acerca de este proyecto"):
        st.markdown("""
        Este proyecto utiliza el modelo <strong>YOLOv8</strong> entrenado con un dataset de se√±ales de tr√°fico.
        Est\u00E1 dise\u00F1ado para funcionar en aplicaciones interactivas como esta, permitiendo detecci\u00F3n en im\u00E1genes de forma r\u00E1pida.
        Se implement√≥ usando <strong>Streamlit</strong> para la interfaz y <strong>OpenCV</strong> para el procesamiento de im\u00E1genes.
        """, unsafe_allow_html=True)

# El bloque 'with st.container():' cierra autom√°ticamente el contenedor